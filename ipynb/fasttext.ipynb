{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import fasttext\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FastText in module gensim.models.fasttext:\n",
      "\n",
      "class FastText(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Class for training, using and evaluating word representations learned using method\n",
      " |  described in [1]_ aka Fasttext.\n",
      " |  \n",
      " |  The model can be stored/loaded via its :meth:`~gensim.models.fasttext.FastText.save()` and\n",
      " |  :meth:`~gensim.models.fasttext.FastText.load()` methods, or loaded in a format compatible with the original\n",
      " |  fasttext implementation via :meth:`~gensim.models.fasttext.FastText.load_fasttext_format()`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FastText\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use self.wv.__contains__() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.__contains__`\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use self.wv.__getitem__() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.__getitem__`\n",
      " |  \n",
      " |  __init__(self, sentences=None, sg=0, hs=0, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=())\n",
      " |      Initialize the model from an iterable of `sentences`. Each sentence is a\n",
      " |      list of words (unicode strings) that will be used for training.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      sg : int {1, 0}\n",
      " |          Defines the training algorithm. If 1, skip-gram is used, otherwise, CBOW is employed.\n",
      " |      size : int\n",
      " |          Dimensionality of the feature vectors.\n",
      " |      window : int\n",
      " |          The maximum distance between the current and predicted word within a sentence.\n",
      " |      alpha : float\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      min_count : int\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      max_vocab_size : int\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      sample : float\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      workers : int\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      hs : int {1,0}\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      cbow_mean : int {1,0}\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      hashfxn : function\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |          of the model.\n",
      " |      sorted_vocab : int {1,0}\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |      batch_words : int\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      min_n : int\n",
      " |          Min length of char ngrams to be used for training word representations.\n",
      " |      max_n : int\n",
      " |          Max length of char ngrams to be used for training word representations. Set `max_n` to be\n",
      " |          lesser than `min_n` to avoid char ngrams being used.\n",
      " |      word_ngrams : int {1,0}\n",
      " |          If 1, uses enriches word vectors with subword(ngrams) information.\n",
      " |          If 0, this is equivalent to word2vec.\n",
      " |      bucket : int\n",
      " |          Character ngrams are hashed into a fixed number of buckets, in order to limit the\n",
      " |          memory usage of the model. This option specifies the number of buckets used by the model.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a `FastText` model\n",
      " |      \n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |      >>>\n",
      " |      >>> model = FastText(sentences, min_count=1)\n",
      " |      >>> say_vector = model['say']  # get vector for word\n",
      " |      >>> of_vector = model['of']  # get vector for out-of-vocab word\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |  \n",
      " |  build_vocab(self, sentences, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      Each sentence must be a list of unicode strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      keep_raw_vocab : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      trim_rule : function\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |          of the model.\n",
      " |      progress_per : int\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      Train a model and update vocab for online training\n",
      " |      \n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> sentences_1 = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |      >>> sentences_2 = [[\"dude\", \"say\", \"wazzup!\"]]\n",
      " |      >>>\n",
      " |      >>> model = FastText(min_count=1)\n",
      " |      >>> model.build_vocab(sentences_1)\n",
      " |      >>> model.train(sentences_1, total_examples=model.corpus_count, epochs=model.iter)\n",
      " |      >>> model.build_vocab(sentences_2, update=True)\n",
      " |      >>> model.train(sentences_2, total_examples=model.corpus_count, epochs=model.iter)\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Removes all L2-normalized vectors for words from the model.\n",
      " |      You will have to recompute them using init_sims method.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      init_sims() resides in KeyedVectors because it deals with syn0 mainly, but because syn1 is not an attribute\n",
      " |      of KeyedVectors, it has to be deleted in this class, and the normalizing of syn0 happens inside of KeyedVectors\n",
      " |  \n",
      " |  load_binary_data(self, encoding='utf8')\n",
      " |      Loads data from the output binary file created by FastText training\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model. This saved model can be loaded again using :func:`~gensim.models.fasttext.FastText.load`,\n",
      " |      which supports online training and getting vectors for out-of-vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  struct_unpack(self, file_handle, fmt)\n",
      " |  \n",
      " |  train(self, sentences, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences (can be a once-only generator stream).\n",
      " |      For FastText, each sentence must be a list of unicode strings.\n",
      " |      \n",
      " |      To support linear learning-rate decay from (initial) alpha to min_alpha, and accurate\n",
      " |      progress-percentage logging, either total_examples (count of sentences) or total_words (count of\n",
      " |      raw words in sentences) **MUST** be provided (if the corpus is the same as was provided to\n",
      " |      :meth:`~gensim.models.fasttext.FastText.build_vocab()`, the count of examples in that corpus\n",
      " |      will be available in the model's :attr:`corpus_count` property).\n",
      " |      \n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case,\n",
      " |      where :meth:`~gensim.models.fasttext.FastText.train()` is only called once,\n",
      " |      the model's cached `iter` value should be supplied as `epochs` value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float\n",
      " |          Initial learning rate.\n",
      " |      end_alpha : float\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |      word_count : int\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |      >>>\n",
      " |      >>> model = FastText(min_count=1)\n",
      " |      >>> model.build_vocab(sentences)\n",
      " |      >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Loads a previously saved `FastText` model. Also see `save()`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `~gensim.models.fasttext.FastText`\n",
      " |          Returns the loaded model as an instance of :class: `~gensim.models.fasttext.FastText`.\n",
      " |  \n",
      " |  load_fasttext_format(model_file, encoding='utf8') from builtins.type\n",
      " |      Load the input-hidden weight matrix from the fast text output files.\n",
      " |      \n",
      " |      Note that due to limitations in the FastText API, you cannot continue training\n",
      " |      with a model loaded this way, though you can query for word similarity etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model_file : str\n",
      " |          Path to the FastText output files.\n",
      " |          FastText outputs two model files - `/path/to/model.vec` and `/path/to/model.bin`\n",
      " |          Expected value for this example: `/path/to/model` or `/path/to/model.bin`,\n",
      " |          as gensim requires only `.bin` file to load entire fastText model.\n",
      " |      encoding : str\n",
      " |          Specifies the encoding.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `~gensim.models.fasttext.FastText`\n",
      " |          Returns the loaded model as an instance of :class: `~gensim.models.fasttext.FastText`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  bucket\n",
      " |  \n",
      " |  max_n\n",
      " |  \n",
      " |  min_n\n",
      " |  \n",
      " |  num_ngram_vectors\n",
      " |  \n",
      " |  syn0_ngrams_lockf\n",
      " |  \n",
      " |  syn0_vocab_lockf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      Build model vocabulary from a passed dictionary that contains (word,word count).\n",
      " |      Words must be of type unicode strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict\n",
      " |          Word,Word_Count dictionary.\n",
      " |      keep_raw_vocab : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      corpus_count : int\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |          of the model.\n",
      " |      update : bool\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from gensim.models import Word2Vec\n",
      " |      >>>\n",
      " |      >>> model= Word2Vec()\n",
      " |      >>> model.build_vocab_from_freq({\"Word1\": 15, \"Word2\": 20})\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated. Use self.wv.doesnt_match() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated. Use self.wv.evaluate_word_pairs() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated. Use self.wv.most_similar() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated. Use self.wv.most_similar_cosmul() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated. Use self.wv.n_similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_vector() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_word() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated. Use self.wv.similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated. Use self.wv.wmdistance() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fasttext.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satetsu-gpu/anaconda3/envs/nlp/lib/python3.5/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/media/satetsu-gpu/Data/program/project/NLP/NLP_test')\n",
    "\n",
    "f = open(\"data/data.txt\", \"r\")\n",
    "text = f.read()\n",
    "sentences = [s.split(\" \") for s in text.split(\"\\n\")]\n",
    "\n",
    "model = fasttext.FastText(min_count=1)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save(\"model/fasttext_gensim_2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
